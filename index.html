<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Egocentric Manipulation Interface</title>
  <!-- <h1>Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations</h1> -->
  <!-- <link rel="icon" href="./data/favicon.png" type="image/png"> -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <style>
    :root {
      /*
        To change the overall background color of the website, adjust --bg below.
        For a subtle off-white, try #f9f9f9, #f7f7fa, or #fcfcfc.
      */
      --accent: #3478f6;
      --bg: #f6f5f2;
      --text: #111;
      --sidebar-bg: #f6f5f2;
      --sidebar-text: #555;
      --sidebar-active: var(--accent);
      --title-font: 'Inter', system-ui, sans-serif;
      --body-font: 'Inter', system-ui, sans-serif;
      --sidebar-width: max(20vw, 180px);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: var(--body-font);
      background-color: var(--bg);
      color: var(--text);
      display: flex;
      scroll-behavior: smooth;
      font-size: 1.08rem;
      line-height: 1.6;
    }

    .sidebar {
      width: var(--sidebar-width);
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      background: var(--sidebar-bg);
      border: none;
      padding: 2rem 1rem;
      text-align: right;
    }

    .sidebar h2 {
      font-size: 1.25rem;
      margin-bottom: 1rem;
      color: var(--text);
    }

    .sidebar ul {
      list-style: none;
      padding: 0;
    }

    .sidebar ul li {
      margin-bottom: 0.75rem;
    }

    .sidebar a {
      text-decoration: none;
      color: var(--sidebar-text);
      font-weight: 500;
      transition: color 0.25s, font-weight 0.25s, text-shadow 0.25s;
    }

    .sidebar a.active,
    .sidebar a:hover {
      color: var(--sidebar-active);
      font-weight: 700;
      text-shadow: 0 2px 8px rgba(52,120,246,0.10), 0 1px 0 #fff;
      text-decoration: none;
    }

    main {
      /* margin-left: var(--sidebar-width); */
      margin: auto;
      width: calc(80% - var(--sidebar-width));
      max-width: 1000px;
      padding: 0 2rem;
      box-sizing: border-box;
    }

    section {
      margin-bottom: 3rem;
    }

    h1, h2 {
      color: #1a1a1a;
      font-family: var(--title-font);
      font-weight: 700;
      letter-spacing: -0.5px;
    }

    h1 {
      font-size: 2.1rem;
      margin-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.35rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }

    .main-title {
      font-size: 3rem;
      font-weight: 800;
      font-family: var(--title-font);
      margin-bottom: 0.2rem;
      margin-top: 0.2rem;
      line-height: 1.1;
      letter-spacing: -1px;
      text-align: left;
      color: #111;
    }

    .main-title .gradient {
      background: linear-gradient(90deg, #60d366 0%, #7221ff 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      color: transparent;
      font-weight: 900;
    }

    .subtitle {
      font-size: 1.95rem;
      font-weight: 500;
      color: #222;
      margin-bottom: 1.1rem;
      line-height: 1.3;
      text-align: left;
    }

    .video-carousel {
      display: flex;
      overflow-x: auto;
      gap: 0.75rem;
      padding: 0.5rem 4px;
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      box-sizing: border-box;
      white-space: nowrap;
    }

    .video-carousel video {
      width: 240px;
      border-radius: 4px;
      border: 1px solid #ccc;
      flex-shrink: 0;
    }

    .iframe-pair {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      margin-top: 1rem;
      justify-content: center;
    }

    .iframe-pair iframe {
      flex: 1 1 48%;
      height: 400px;
      border: 1px solid #ccc;
      border-radius: 4px;
    }

    img {
      max-width: 100%;
      border-radius: 4px;
      box-shadow: 0 0 8px rgba(0,0,0,0.1);
      /* border: 1px solid #ccc; */
    }

    pre {
      background: #23272e;
      color: #f8f8f2;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.92em;
      border: 1px solid #181a1f;
    }

    code {
      color: #f8f8f2;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 1rem;
    }

    video, iframe {
      box-shadow: 0 0 8px rgba(0,0,0,0.1);
    }

    .selected-thumb {
      box-shadow: 0 0 8px rgba(52,120,246,0.18), 0 0 8px rgba(0,0,0,0.12);
      z-index: 2;
    }

    @media (max-width: 900px) {
      .sidebar {
        display: none;
      }
      :root {
        --sidebar-width: 0;
      }
      main {
        margin-left: 0;
        max-width: 100vw;
        width: 100vw;
        padding: 0;
      }
      .main-content {
        max-width: 100vw;
        width: 100vw;
        padding: 0 8px;
      }
      .video-carousel {
        max-width: 100vw;
        padding: 0.5rem 8px;
      }
      .main-title {
        font-size: 2.4rem !important;
      }
    }

    .main-content {
      max-width: 100% !important;
      width: 100vw;
      padding: 0 30px;
    }

    .iframe-container {
      position: relative;
      width: 100%;
      max-width: 100%;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    .click-and-move-overlay {
      position: absolute;
      top: 8%;
      left: 50%;
      transform: translate(-50%, -50%);
      background: rgba(255,255,255,0.85);
      border-radius: 4px;
      padding: 0.5rem 1.2rem;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      z-index: 10;
      pointer-events: none;
      display: flex;
      align-items: center;
      font-size: 1.15rem;
      font-weight: 550;
      color: #222;
      gap: 0.5rem;
    }
    .click-and-move-overlay .inline-image {
      height: 1.5em;
      vertical-align: middle;
      margin: 0 0.2em;
    }

    .carousel-hint {
      text-align: center;
      color: #666;
      font-size: 0.98rem;
      margin-bottom: 0.3rem;
      margin-top: 0.2rem;
      font-weight: 500;
      letter-spacing: 0.01em;
    }

    .carousel-wrapper {
      position: relative;
      width: 100%;
      max-width: min(1000px, calc(100vw - 240px));
      margin: 0 auto;
      display: flex;
      align-items: center;
    }
    .carousel-arrow {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      background: rgba(255,255,255,0.85);
      border: 1px solid #ccc;
      border-radius: 50%;
      width: 36px;
      height: 36px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.5rem;
      color: #3478f6;
      cursor: pointer;
      z-index: 5;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      transition: background 0.2s;
      user-select: none;
    }
    .carousel-arrow:hover {
      background: #f0f4ff;
    }
    .carousel-arrow.left {
      left: -18px;
    }
    .carousel-arrow.right {
      right: -18px;
    }

  </style>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const links = document.querySelectorAll('.sidebar a');
      const sections = Array.from(document.querySelectorAll('main section'));

      function updateActiveLink() {
        const scrollY = window.scrollY + 100;
        let current = sections[0];

        for (const section of sections) {
          if (section.offsetTop <= scrollY) {
            current = section;
          }
        }

        links.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === `#${current.id}`) {
            link.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', updateActiveLink);
      updateActiveLink();

      // Main iframe swap logic for thumbnails
      const mainIframe = document.getElementById('main-iframe');
      const thumbnailVideos = document.querySelectorAll('#iframe-thumbnails video');
      const iframeSrcs = [
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251022_210201.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251024_112052.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251024_112206.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251024_112328.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251024_112608.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000',
      ];
      if (mainIframe && thumbnailVideos.length === iframeSrcs.length) {
        thumbnailVideos.forEach((vid, idx) => {
          vid.addEventListener('click', () => {
            mainIframe.src = iframeSrcs[idx];
            // Highlight the selected video
            thumbnailVideos.forEach(v => { v.style.outline = ''; v.classList.remove('selected-thumb'); });
            vid.style.outline = '3px solid var(--accent)';
            vid.classList.add('selected-thumb');
          });
        });
        // Highlight the first by default
        thumbnailVideos[0].style.outline = '3px solid var(--accent)';
        thumbnailVideos[0].classList.add('selected-thumb');
      }

    // Physical rollouts main video selector logic
    const mainPhysicalVideo = document.getElementById('main-physical-video');
    const physicalThumbnails = document.querySelectorAll('#physical-video-thumbnails video');
    if (mainPhysicalVideo && physicalThumbnails.length > 0) {
      physicalThumbnails.forEach((vid, idx) => {
        vid.addEventListener('click', () => {
          const src = vid.getAttribute('data-video');
          if (src) {
            mainPhysicalVideo.querySelector('source').src = src;
            mainPhysicalVideo.load();
            mainPhysicalVideo.play();
            // Highlight selected
            physicalThumbnails.forEach(v => v.style.outline = '');
            vid.style.outline = '3px solid var(--accent)';
          }
        });
      });
      // Highlight the first by default
      physicalThumbnails[0].style.outline = '3px solid var(--accent)';
    }
  });
  </script>
  <body>
    <nav class="sidebar">
      
      <ul>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#realrollouts">Real World Policy Rollouts</a></li>
  <li><a href="#hardware">EgoMI Device</a></li>
  <li><a href="#sparks">Spatial Aware Robust Keyframe Selection (SPARKS)</a></li>
  <li><a href="#randomization">Experimental Randomization Distribution</a></li>
  <!-- <li><a href="#bibtex">BibTeX</a></li> -->
      </ul>
    </nav>
    <main>
      <div class="main-content">
      <section id="title-block">
        <div style="margin-top: 20px;"></div>
        <div class="main-title"><span class="gradient">Egocentric</span><span> Manipulation</span><span> Interface</span></div>
        <div class="subtitle">Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations</div>
        <p style="font-size: 1.1rem; margin-bottom: 0.1rem;">Anonymous Authors, Under Peer Review</p>
        <!-- <p style="font-size: 1rem; font-style: italic; margin-bottom: 0.1rem;">Under Review CoRL 2025</p> -->
      </section>
  
      <section id="overview-video">
        <div style="display: flex; justify-content: center; width: 100%; margin: 1rem 0;">
          <video autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
            <source src="data/egomi-teaser-compress.mp4" type="video/mp4">
          </video>
        </div>
        <div style="max-width: 900px; margin: 0 auto 1.5rem auto; font-size: 1.15rem; color: #222; text-align: center;">
          <b>EgoMI</b> is a scalable framework for collecting and deploying egocentric human demonstration data to train and retarget whole-body + active vision manipulation policies - without requiring robot hardware for teleoperation.
        </div>
        <div style="text-align: center; margin-bottom: 2.5rem;">
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[Paper (coming soon)]</a>
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[arXiv (coming soon)]</a>
          <a href="#" style="color: #3478f6; text-decoration: none; font-weight: 600;">[Code (coming soon)]</a>
        </div>
      </section>
      <section id="abstract">
        <!-- <h1>Abstract</h1>  -->
        <h1><span class="toggle-btn" onclick="toggleAbstract()">Abstract &#9662</span><span id="expand-hint" style="font-size: 0.8rem; margin-left: 8px; color: #424242;">(click to expand)</span></h1>
        <div id="abstract-content" style="display: none; max-height: 0; overflow: hidden; transition: max-height 0.5s ease-out;">
        Imitation learning from human demonstrations
        offers a promising approach for robot skill acquisition, but
        egocentric human data introduces fundamental challenges due
        to the embodiment gap. During manipulation, humans actively
        coordinate head and hand movements, continuously reposition
        their viewpoint and use pre-action visual fixation search
        strategies to locate relevant objects. These behaviors create
        dynamic, task-driven head motions that static robot sensing
        systems cannot replicate, leading to a significant distribution
        shift that degrades policy performance. We present <b>EgoMI</b>, a
        framework that captures synchronized end-effector and active
        head trajectories during manipulation tasks, resulting in data
        that can be retargeted to compatible semi-humanoid robot
        embodiments. To handle rapid and wide-spanning head view-
        point changes, we introduce a memory-augmented policy that
        selectively incorporates historical observations. We evaluate our
        approach on a bimanual robot equipped with an actuated camera
        head and find that policies with explicit head-motion modeling
        consistently outperform baseline methods. Results suggest that
        coordinated hand-eye learning with EgoMI effectively bridges
        the human-robot embodiment gap for robust imitation learning
        on semi-humanoid embodiments.
        </div>
        <script>
          function toggleAbstract() {
            const content = document.getElementById('abstract-content');
            const btn = document.querySelector('.toggle-btn');
            const hint = document.getElementById('expand-hint');
            if (content.style.display === 'none') {
              content.style.display = 'block';
              setTimeout(() => {
                content.style.maxHeight = content.scrollHeight + "px";
              }, 10);
              btn.innerHTML = 'Abstract &#9652';
              hint.style.display = 'none'; // Hide the hint after expanding
            } else {
              content.style.maxHeight = '0';
              setTimeout(() => {
                content.style.display = 'none';
              }, 500); // Wait for transition to complete
              btn.innerHTML = 'Abstract &#9662';
            }
          }
        </script>
      </section>
  
      <section id="realrollouts">
          <h2>Real World Policy Rollouts</h2>
          <div style="display: flex; justify-content: center; width: 100%; margin-bottom: 1rem;">
            <video id="main-physical-video" autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
              <source src="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-carousel" id="physical-video-thumbnails">
            <video data-video="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" autoplay muted loop playsinline src="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" style="cursor:pointer;"></video>
            <video data-video="data/Rollout_Videos/EgoMI_TableTop_1_Compress.mp4" autoplay muted loop playsinline src="data/Rollout_Videos/EgoMI_TableTop_1_Compress.mp4" style="cursor:pointer;"></video>
            <video data-video="data/Rollout_Videos/EgoMISorting.mp4" autoplay muted loop playsinline src="data/Rollout_Videos/EgoMISorting.mp4" style="cursor:pointer;"></video>
          </div>

          <div style="display: flex; flex-direction: column; align-items: center; width: 100%;">
            <div class="iframe-container" style="margin-bottom: 0.75rem;">
              <iframe id="main-iframe" src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/egomi/recording_20251022_210201.viser&initialCameraPosition=-0.480,1.004,1.674&initialCameraLookAt=0.212,0.032,0.923&initialCameraUp=0.000,0.000,1.000" style="width: 100%; max-width: 100%; height: 600px; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);"></iframe>
              <div class="click-and-move-overlay">
                <img src="data/drag_icon.png" alt="" class="inline-image">
                Click and move me!
                <img src="data/drag_icon.png" alt="" class="inline-image">
              </div>
            </div>
            <div class="carousel-wrapper">
              <div class="carousel-arrow left" id="iframe-carousel-left">&#8592;</div>
              <div class="video-carousel" id="iframe-thumbnails">
                <video data-iframe="0" autoplay muted loop playsinline src="data/Rollout_Videos/egomi1_top_camera-images-left_rgb.mp4" style="cursor:pointer;"></video>
                <video data-iframe="1" autoplay muted loop playsinline src="data/Rollout_Videos/egomi2_top_camera-images-left_rgb.mp4" style="cursor:pointer;"></video>
                <video data-iframe="2" autoplay muted loop playsinline src="data/Rollout_Videos/egomi3_top_camera-images-left_rgb.mp4" style="cursor:pointer;"></video>
                <video data-iframe="3" autoplay muted loop playsinline src="data/Rollout_Videos/egomi4_top_camera-images-left_rgb.mp4" style="cursor:pointer;"></video>
                <video data-iframe="4" autoplay muted loop playsinline src="data/Rollout_Videos/egomi5_top_camera-images-left_rgb.mp4" style="cursor:pointer;"></video>

              </div>
              <div class="carousel-arrow right" id="iframe-carousel-right">&#8594;</div>
            </div>
            <div class="carousel-hint">Click a thumbnail to change the interactive view above.</div>
          </div>

      </section>
  
      <section id="hardware">
        <h2>EgoMI Device</h2>
        <img src="data/egomi_device.png" alt="">
        The EgoMI device records egocentric human demonstrations, capturing synchronized head, hand, and visual data to bridge the human-robot embodiment gap. It combines a Meta Quest 3S headset for head and hand tracking with a camera mounted above the headset for egocentric head observation and neck action retargeting. Each hand controller includes a small wrist-mounted camera and a standard mechanical flange mount for a robot gripper. During collection, the controller triggers actuate the drive-by-wire grippers, letting any user perform real bimanual manipulation tasks naturally. The resulting dataset consists of synchronized head and hand motion, and video streams that can be retargeted directly to robots with no teleoperation or on-embodiment data required.      
      </section>
  
      <section id="sparks">
        <h2>Spatial Aware Robust Keyframe Selection (SPARKS)</h2>
        SPARKS is a lightweight training-free keyframe selection algorithm that equips imitation-learning policies with spatial memory. Human head motion produces rapid viewpoint shifts, causing critical task information to appear under past perspectives. SPARKS scores and selects keyframes based on viewpoint novelty, temporal recency, and motion smoothness, retaining only the most informative head-camera images. This memory buffer provides the policy with essential spatial context across timeâ€”without recurrent or learned models. SPARKS allows robots to recall past viewpoints during manipulation, yielding more stable, context-aware performance under occlusion, motion, and long-horizon tasks.
      </section>
      
      <section id="randomization">
        <h2>Experimental Randomization Distribution</h2>
        
        <img src="data/randomization.png" alt="">
      </section>


      <footer>
      &copy; Webpage designed by the EgoMI team, inspired by the <a href="https://www.videomimic.net/">VideoMimic</a> and <a href="https://www.real2render2real.com/">Real2Render2Real</a> website.
    </footer>
    </div>
  </main>
</body>
</html>
