<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Egocentric Manipulation Interface</title>
  <!-- <h1>Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations</h1> -->
  <!-- <link rel="icon" href="./data/favicon.png" type="image/png"> -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <style>
    :root {
      --accent: #3478f6;
      --bg: #ffffff;
      --text: #111;
      --sidebar-bg: #f8f9fa;
      --sidebar-text: #555;
      --sidebar-active: var(--accent);
      --title-font: 'Inter', system-ui, sans-serif;
      --body-font: 'Inter', system-ui, sans-serif;
      --sidebar-width: max(20vw, 180px);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: var(--body-font);
      background-color: var(--bg);
      color: var(--text);
      display: flex;
      scroll-behavior: smooth;
      font-size: 1.08rem;
      line-height: 1.6;
    }

    .sidebar {
      width: var(--sidebar-width);
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      background: #fff;
      border: none;
      padding: 2rem 1rem;
      text-align: right;
    }

    .sidebar h2 {
      font-size: 1.25rem;
      margin-bottom: 1rem;
      color: var(--text);
    }

    .sidebar ul {
      list-style: none;
      padding: 0;
    }

    .sidebar ul li {
      margin-bottom: 0.75rem;
    }

    .sidebar a {
      text-decoration: none;
      color: var(--sidebar-text);
      font-weight: 500;
      transition: color 0.25s, font-weight 0.25s, text-shadow 0.25s;
    }

    .sidebar a.active,
    .sidebar a:hover {
      color: var(--sidebar-active);
      font-weight: 700;
      text-shadow: 0 2px 8px rgba(52,120,246,0.10), 0 1px 0 #fff;
      text-decoration: none;
    }

    main {
      /* margin-left: var(--sidebar-width); */
      margin: auto;
      width: calc(80% - var(--sidebar-width));
      max-width: 1000px;
      padding: 0 2rem;
      box-sizing: border-box;
    }

    section {
      margin-bottom: 3rem;
    }

    h1, h2 {
      color: #1a1a1a;
      font-family: var(--title-font);
      font-weight: 700;
      letter-spacing: -0.5px;
    }

    h1 {
      font-size: 2.1rem;
      margin-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.35rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }

    .main-title {
      font-size: 4rem;
      font-weight: 800;
      font-family: var(--title-font);
      margin-bottom: 0.2rem;
      line-height: 1.1;
      letter-spacing: -1px;
      text-align: left;
      color: #111;
    }

    .main-title .gradient {
      background: linear-gradient(90deg, #68d360 0%, #7221ff 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      color: transparent;
      font-weight: 900;
    }

    .subtitle {
      font-size: 1.95rem;
      font-weight: 500;
      color: #222;
      margin-bottom: 1.1rem;
      line-height: 1.3;
      text-align: left;
    }

    .video-carousel {
      display: flex;
      overflow-x: auto;
      gap: 0.75rem;
      padding: 0.5rem 4px;
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      box-sizing: border-box;
      white-space: nowrap;
    }

    .video-carousel video {
      width: 240px;
      border-radius: 4px;
      border: 1px solid #ccc;
      flex-shrink: 0;
    }

    .iframe-pair {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      margin-top: 1rem;
      justify-content: center;
    }

    .iframe-pair iframe {
      flex: 1 1 48%;
      height: 400px;
      border: 1px solid #ccc;
      border-radius: 4px;
    }

    img {
      max-width: 100%;
      border-radius: 4px;
      /* border: 1px solid #ccc; */
    }

    pre {
      background: #f0f0f0;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.92em;
    }

    code {
      font-size: 0.92em;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 1rem;
    }

    @media (max-width: 900px) {
      .sidebar {
        display: none;
      }
      :root {
        --sidebar-width: 0;
      }
      main {
        margin-left: 0;
        width: 100%;
        padding: 0;
      }
      .main-content {
        max-width: 100vw;
        padding: 0 8px;
      }
      .video-carousel {
        max-width: 100vw;
        padding: 0.5rem 8px;
      }
    }

    .main-content {
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      overflow: visible;
    }
  </style>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const links = document.querySelectorAll('.sidebar a');
      const sections = Array.from(document.querySelectorAll('main section'));

      function updateActiveLink() {
        const scrollY = window.scrollY + 100;
        let current = sections[0];

        for (const section of sections) {
          if (section.offsetTop <= scrollY) {
            current = section;
          }
        }

        links.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === `#${current.id}`) {
            link.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', updateActiveLink);
      updateActiveLink();

    // Physical rollouts main video selector logic
    const mainPhysicalVideo = document.getElementById('main-physical-video');
    const physicalThumbnails = document.querySelectorAll('#physical-video-thumbnails video');
    if (mainPhysicalVideo && physicalThumbnails.length > 0) {
      physicalThumbnails.forEach((vid, idx) => {
        vid.addEventListener('click', () => {
          const src = vid.getAttribute('data-video');
          if (src) {
            mainPhysicalVideo.querySelector('source').src = src;
            mainPhysicalVideo.load();
            mainPhysicalVideo.play();
            // Highlight selected
            physicalThumbnails.forEach(v => v.style.outline = '');
            vid.style.outline = '3px solid var(--accent)';
          }
        });
      });
      // Highlight the first by default
      physicalThumbnails[0].style.outline = '3px solid var(--accent)';
    }
  });
  </script>
  <body>
    <nav class="sidebar">
      
      <ul>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#realrollouts">Real World Policy Rollouts</a></li>
  <li><a href="#hardware">EgoMI Device</a></li>
  <li><a href="#sparks">Spatial Aware Robust Keyframe Selection (SPARKS)</a></li>
  <li><a href="#randomization">Experimental Randomization Distribution</a></li>
  <!-- <li><a href="#bibtex">BibTeX</a></li> -->
      </ul>
    </nav>
    <main>
      <div class="main-content">
      <section id="title-block">
        <div style="margin-top: 20px;"></div>
        <div class="main-title"><span class="gradient">Egocentric</span><span> Manipulation</span><span> Interface</span></div>
        <div class="subtitle">Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations</div>
        <p style="font-size: 1.1rem; margin-bottom: 0.1rem;">Anonymous Authors, Under Peer Review</p>
        <!-- <p style="font-size: 1rem; font-style: italic; margin-bottom: 0.1rem;">Under Review CoRL 2025</p> -->
      </section>
  
      <section id="overview-video">
        <div style="display: flex; justify-content: center; width: 100%; margin: 1rem 0;">
          <video autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
            <source src="data/twitter-final-compress.mov" type="video/mp4">
          </video>
        </div>
        <div style="max-width: 900px; margin: 0 auto 1.5rem auto; font-size: 1.15rem; color: #222; text-align: center;">
          <b>EgoMI</b> is a scalable framework for collecting and deploying egocentric human demonstration data to train and retarget whole-body + active vision generalist manipulation policies - without requiring robot hardware for teleoperation.
        </div>
        <div style="text-align: center; margin-bottom: 2.5rem;">
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[Paper (coming soon)]</a>
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[arXiv (coming soon)]</a>
          <a href="#" style="color: #3478f6; text-decoration: none; font-weight: 600;">[Code (coming soon)]</a>
        </div>
      </section>
      <section id="abstract">
        <h1>Abstract</h1> 
        Imitation learning from human demonstrations
        offers a promising approach for robot skill acquisition, but
        egocentric human data introduces fundamental challenges due
        to the embodiment gap. During manipulation, humans actively
        coordinate head and hand movements, continuously reposition
        their viewpoint and use pre-action visual fixation search
        strategies to locate relevant objects. These behaviors create
        dynamic, task-driven head motions that static robot sensing
        systems cannot replicate, leading to a significant distribution
        shift that degrades policy performance. We present <b>EgoMI</b>, a
        framework that captures synchronized end-effector and active
        head trajectories during manipulation tasks, resulting in data
        that can be retargeted to compatible semi-humanoid robot
        embodiments. To handle rapid and wide-spanning head view-
        point changes, we introduce a memory-augmented policy that
        selectively incorporates historical observations. We evaluate our
        approach on a bimanual robot equipped with an actuated camera
        head and find that policies with explicit head-motion modeling
        consistently outperform baseline methods. Results suggest that
        coordinated hand-eye learning with EgoMI effectively bridges
        the human-robot embodiment gap for robust imitation learning
        on semi-humanoid embodiments.
      </section>
  
      <section id="realrollouts">
          <h2>Real World Policy Rollouts</h2>
          <div style="display: flex; justify-content: center; width: 100%; margin-bottom: 1rem;">
            <video id="main-physical-video" autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
              <source src="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-carousel" id="physical-video-thumbnails">
            <video data-video="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" autoplay muted loop playsinline src="data/Rollout_Videos/EgoMI_Shelf_Task_1.mp4" style="cursor:pointer;"></video>
            <video data-video="data/Rollout_Videos/EgoMI_TableTop_1_Compress.mp4" autoplay muted loop playsinline src="data/Rollout_Videos/EgoMI_TableTop_1_Compress.mp4" style="cursor:pointer;"></video>
          </div>
      </section>
  
      <section id="hardware">
        <h2>EgoMI Device</h2>
        Work in progress...
      </section>
  
      <section id="sparks">
        <h2>Spatial Aware Robust Keyframe Selection (SPARKS)</h2>
          Work in progress...
      </section>
      
      <section id="randomization">
        <h2>Experimental Randomization Distribution</h2>
        Work in progress...
      </section>


      <footer>
      &copy; Webpage designed by the EgoMI team, inspired by the <a href="https://www.videomimic.net/">VideoMimic</a> and <a href="https://www.real2render2real.com/">Real2Render2Real</a> website.
    </footer>
    </div>
  </main>
</body>
</html>
